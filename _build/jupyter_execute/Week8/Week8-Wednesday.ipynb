{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "243ae36c-dcd7-4a79-82f7-037452b559a5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Week 8 Wednesday\n",
    "\n",
    "[Yuja recording](https://uci.yuja.com/V/Video?v=4446584&node=14938258&a=1900985263&autoplay=1)\n",
    "\n",
    "Before the recording, at the board we went over some different components related to Neural Networks and PyTorch, and especially we went over an example of performing gradient descent.\n",
    "\n",
    "The goal of today's class is to get more comfortable with the various components involved in building and training a neural network using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "f0eb36b3-9e83-4240-a88d-3ea0bce49ed7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3505,
    "execution_start": 1645636684575,
    "source_hash": "aaa7822b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "15b1e673-e2d4-4521-bef0-984d0db8ba16",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ab4cbfec-77e0-4fa9-b5d9-a147b95bda39",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Gradient descent can be used to try to find a minimum of any differentiable function.  (Often it will only find a local minimum, not a global minimum, even if a global minimum exists.)  We usually use gradient descent for very complicated functions, but here we give an example of performing gradient descent to attempt to find a minimum of the function\n",
    "\n",
    "$$\n",
    "f(x,y) = (x-3)^2 + (y+2)^2 + 8.\n",
    "$$\n",
    "\n",
    "We call this function `loss_fn` so that the syntax is the same as what we're used to in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "bfcb90b1-ebce-46c0-9c61-8e5bea9f0891",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1645633781540,
    "source_hash": "e9282212",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = lambda t: (t[0] - 3)**2 + (t[1] + 2)**2 + 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform gradient descent, you need to begin with an initial guess.  We guess (10,10) and then gradually adjust this, hoping to move towards a minimum.  Notice the decimal point after 10... this is a shortcut for telling PyTorch that these should be treated as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "8f673eca-3037-4c5f-a4c8-79500d4e9fa1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 10,
    "execution_start": 1645633855875,
    "source_hash": "6446e83e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([10.,10], requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "95b08da7-abb5-4b92-bcf1-f987d10c1710",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16,
    "execution_start": 1645633874572,
    "source_hash": "2f82c7c2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn([10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "90839987-f3bb-4505-af9f-6e779c622f40",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1645633860985,
    "source_hash": "54809b1c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(201., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "2ba3e3f4-ac82-4d6d-ab21-944229d4d7d8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1645633947376,
    "source_hash": "baf1bcea",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we specified `requires_grad=True` as a keyword argument, we will be able to find gradients of computations involving `a`.  There isn't any gradient yet because we haven't computed one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "cab68ec9-8d1b-471a-953c-ab0f3e94b5ad",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1645634000065,
    "source_hash": "9e258c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a stochastic gradient descent optimizer like usual in PyTorch.  The first input is usually something like `model.parameters()`.  Here we try to use `a` as the first argument.  That is almost right, but we need to put it in a list (or some other type of *iterable*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "ec294756-ae56-415c-94ac-d13b036e553c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15,
    "execution_start": 1645634163821,
    "source_hash": "402fc787",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/sgd.py:95\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, params, lr, momentum, dampening, weight_decay, nesterov)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/optim/optimizer.py:40\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_for_profile()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams argument given to the optimizer should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man iterable of Tensors or dicts, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     42\u001b[0m                     torch\u001b[38;5;241m.\u001b[39mtypename(params))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(a, lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "7a62f9b5-5cee-40f6-8e9d-1de51dbc3ec8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1645634211760,
    "source_hash": "f39e7697",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([a], lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "7bea48d1-9ef9-497d-9d2b-4339cbd2fdc4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1645634267489,
    "source_hash": "cbb90e2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss_fn(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next `optimizer.zero_grad()` is not important yet, but it is good to be in the habit, because otherwise multiple gradient computations will accumulate, and we want to start over each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "10079734-a52f-4100-9c69-ff9dacdf0900",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3,
    "execution_start": 1645634327650,
    "source_hash": "5674e723",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "82a15d63-872b-40ad-ae2b-c247f7c2c9d7",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1645634489470,
    "source_hash": "325dc1fb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the gradient.  This typically uses an algorithm called *backpropagation*, which is where the name `backward` comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "836a9716-e8fe-4a9e-b3b2-08c54d44d02c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1645634499147,
    "source_hash": "f4c34168",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `grad` attribute of `a` has a value.  You should be able to compute this value by hand in this case, since our `loss_fn` is so simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "02f61431-9c0b-4941-940d-6ed5a4412375",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 193,
    "execution_start": 1645634515764,
    "source_hash": "9e258c0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14., 24.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace add a multiple (the learning rate `lr`) of the negative gradient to `a`.  Again, you should be able to compute this by hand in this case.  The formula is\n",
    "\n",
    "$$\n",
    "a \\leadsto a - lr \\cdot \\nabla\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "eb6ebf1c-4c2b-4eb4-af5b-609a73ac4316",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1645634540996,
    "source_hash": "8a03d43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "029dfe9c-2d46-4634-b651-6a4bfe44cfbb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1645634543709,
    "source_hash": "ef4233c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.6000, 7.6000], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "4991d3db-a1d1-470a-8d01-0915a3642e92",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5,
    "execution_start": 1645636692946,
    "source_hash": "e9282212",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = lambda t: (t[0] - 3)**2 + (t[1] + 2)**2 + 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the value of `a` is approaching the minimum (3,-2), and notice how `loss` is approaching the minimum of our `loss_fn`, which is 8.  (The only reason we're using the terms `loss` and `loss_fn` is because those are the terms we usually use in PyTorch.  In this case, `loss_fn` is just an ordinary two-variable function like from Math 2D which we are trying to minimize.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "9d2d2196-d692-4125-a84f-8327c8d33010",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 132,
    "execution_start": 1645636693882,
    "source_hash": "3b5408a1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tensor([8.6000, 7.6000], requires_grad=True)\n",
      "tensor(201., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 1\n",
      "tensor([7.4800, 5.6800], requires_grad=True)\n",
      "tensor(131.5200, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 2\n",
      "tensor([6.5840, 4.1440], requires_grad=True)\n",
      "tensor(87.0528, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 3\n",
      "tensor([5.8672, 2.9152], requires_grad=True)\n",
      "tensor(58.5938, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 4\n",
      "tensor([5.2938, 1.9322], requires_grad=True)\n",
      "tensor(40.3800, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 5\n",
      "tensor([4.8350, 1.1457], requires_grad=True)\n",
      "tensor(28.7232, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 6\n",
      "tensor([4.4680, 0.5166], requires_grad=True)\n",
      "tensor(21.2629, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 7\n",
      "tensor([4.1744, 0.0133], requires_grad=True)\n",
      "tensor(16.4882, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 8\n",
      "tensor([ 3.9395, -0.3894], requires_grad=True)\n",
      "tensor(13.4325, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 9\n",
      "tensor([ 3.7516, -0.7115], requires_grad=True)\n",
      "tensor(11.4768, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 10\n",
      "tensor([ 3.6013, -0.9692], requires_grad=True)\n",
      "tensor(10.2251, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 11\n",
      "tensor([ 3.4810, -1.1754], requires_grad=True)\n",
      "tensor(9.4241, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 12\n",
      "tensor([ 3.3848, -1.3403], requires_grad=True)\n",
      "tensor(8.9114, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 13\n",
      "tensor([ 3.3079, -1.4722], requires_grad=True)\n",
      "tensor(8.5833, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 14\n",
      "tensor([ 3.2463, -1.5778], requires_grad=True)\n",
      "tensor(8.3733, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 15\n",
      "tensor([ 3.1970, -1.6622], requires_grad=True)\n",
      "tensor(8.2389, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 16\n",
      "tensor([ 3.1576, -1.7298], requires_grad=True)\n",
      "tensor(8.1529, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 17\n",
      "tensor([ 3.1261, -1.7838], requires_grad=True)\n",
      "tensor(8.0979, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 18\n",
      "tensor([ 3.1009, -1.8271], requires_grad=True)\n",
      "tensor(8.0626, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 19\n",
      "tensor([ 3.0807, -1.8616], requires_grad=True)\n",
      "tensor(8.0401, grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "a = torch.tensor([10.,10], requires_grad=True)\n",
    "optimizer = torch.optim.SGD([a], lr = 0.1)\n",
    "for i in range(epochs):\n",
    "    loss = loss_fn(a)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch \" + str(i))\n",
    "    print(a)\n",
    "    print(loss)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want `a` to approach the minimum (3,-2) faster, we can make the learning rate bigger, but here is an example of what can go wrong if we make the learning rate too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "2d379066-042a-4234-b54d-c840f738500d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 51,
    "execution_start": 1645636699061,
    "source_hash": "563fd7cb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tensor([-130., -230.], requires_grad=True)\n",
      "tensor(201., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 1\n",
      "tensor([2530., 4330.], requires_grad=True)\n",
      "tensor(69681., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 2\n",
      "tensor([-48010., -82310.], requires_grad=True)\n",
      "tensor(25151960., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 3\n",
      "tensor([ 912250., 1563850.], requires_grad=True)\n",
      "tensor(9.0799e+09, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 4\n",
      "tensor([-17332690., -29713190.], requires_grad=True)\n",
      "tensor(3.2778e+12, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 5\n",
      "tensor([3.2932e+08, 5.6455e+08], requires_grad=True)\n",
      "tensor(1.1833e+15, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 6\n",
      "tensor([-6.2571e+09, -1.0726e+10], requires_grad=True)\n",
      "tensor(4.2717e+17, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 7\n",
      "tensor([1.1888e+11, 2.0380e+11], requires_grad=True)\n",
      "tensor(1.5421e+20, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 8\n",
      "tensor([-2.2588e+12, -3.8723e+12], requires_grad=True)\n",
      "tensor(5.5669e+22, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 9\n",
      "tensor([4.2917e+13, 7.3573e+13], requires_grad=True)\n",
      "tensor(2.0097e+25, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 10\n",
      "tensor([-8.1543e+14, -1.3979e+15], requires_grad=True)\n",
      "tensor(7.2549e+27, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 11\n",
      "tensor([1.5493e+16, 2.6560e+16], requires_grad=True)\n",
      "tensor(2.6190e+30, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 12\n",
      "tensor([-2.9437e+17, -5.0464e+17], requires_grad=True)\n",
      "tensor(9.4546e+32, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 13\n",
      "tensor([5.5930e+18, 9.5881e+18], requires_grad=True)\n",
      "tensor(3.4131e+35, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 14\n",
      "tensor([-1.0627e+20, -1.8217e+20], requires_grad=True)\n",
      "tensor(1.2321e+38, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 15\n",
      "tensor([2.0191e+21, 3.4613e+21], requires_grad=True)\n",
      "tensor(inf, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 16\n",
      "tensor([-3.8363e+22, -6.5765e+22], requires_grad=True)\n",
      "tensor(inf, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 17\n",
      "tensor([7.2889e+23, 1.2495e+24], requires_grad=True)\n",
      "tensor(inf, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 18\n",
      "tensor([-1.3849e+25, -2.3741e+25], requires_grad=True)\n",
      "tensor(inf, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 19\n",
      "tensor([2.6313e+26, 4.5108e+26], requires_grad=True)\n",
      "tensor(inf, grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "a = torch.tensor([10.,10], requires_grad=True)\n",
    "optimizer = torch.optim.SGD([a], lr = 10)\n",
    "for i in range(epochs):\n",
    "    loss = loss_fn(a)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch \" + str(i))\n",
    "    print(a)\n",
    "    print(loss)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example for what seems to be a good choice of `lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "a5ec569c-3fa6-4e22-aafd-5ba7ae8d5af2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 41,
    "execution_start": 1645636748614,
    "source_hash": "73b17193",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "tensor([6.5000, 4.0000], requires_grad=True)\n",
      "tensor(201., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 1\n",
      "tensor([4.7500, 1.0000], requires_grad=True)\n",
      "tensor(56.2500, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 2\n",
      "tensor([ 3.8750, -0.5000], requires_grad=True)\n",
      "tensor(20.0625, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 3\n",
      "tensor([ 3.4375, -1.2500], requires_grad=True)\n",
      "tensor(11.0156, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 4\n",
      "tensor([ 3.2188, -1.6250], requires_grad=True)\n",
      "tensor(8.7539, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 5\n",
      "tensor([ 3.1094, -1.8125], requires_grad=True)\n",
      "tensor(8.1885, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 6\n",
      "tensor([ 3.0547, -1.9062], requires_grad=True)\n",
      "tensor(8.0471, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 7\n",
      "tensor([ 3.0273, -1.9531], requires_grad=True)\n",
      "tensor(8.0118, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 8\n",
      "tensor([ 3.0137, -1.9766], requires_grad=True)\n",
      "tensor(8.0029, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 9\n",
      "tensor([ 3.0068, -1.9883], requires_grad=True)\n",
      "tensor(8.0007, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 10\n",
      "tensor([ 3.0034, -1.9941], requires_grad=True)\n",
      "tensor(8.0002, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 11\n",
      "tensor([ 3.0017, -1.9971], requires_grad=True)\n",
      "tensor(8.0000, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 12\n",
      "tensor([ 3.0009, -1.9985], requires_grad=True)\n",
      "tensor(8.0000, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 13\n",
      "tensor([ 3.0004, -1.9993], requires_grad=True)\n",
      "tensor(8.0000, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 14\n",
      "tensor([ 3.0002, -1.9996], requires_grad=True)\n",
      "tensor(8.0000, grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 15\n",
      "tensor([ 3.0001, -1.9998], requires_grad=True)\n",
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 16\n",
      "tensor([ 3.0001, -1.9999], requires_grad=True)\n",
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 17\n",
      "tensor([ 3.0000, -2.0000], requires_grad=True)\n",
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 18\n",
      "tensor([ 3.0000, -2.0000], requires_grad=True)\n",
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "\n",
      "Epoch 19\n",
      "tensor([ 3.0000, -2.0000], requires_grad=True)\n",
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "a = torch.tensor([10.,10], requires_grad=True)\n",
    "optimizer = torch.optim.SGD([a], lr = 0.25)\n",
    "for i in range(epochs):\n",
    "    loss = loss_fn(a)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch \" + str(i))\n",
    "    print(a)\n",
    "    print(loss)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "bcba8955-3256-4364-bc42-d90956a82373",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}